{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/annanyapr/composable-sft.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SG97cpGDL7f",
        "outputId": "34ac8f25-a840-44d6-f14b-d4103f01aac8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'composable-sft' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnzi1pMTDGSn",
        "outputId": "4729302d-b041-4cd9-87e1-f359b8acc97d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/composable-sft && git pull https://github.com/annanyapr/composable-sft.git && cd -"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofBrUwl1R65s",
        "outputId": "693ba586-0607-44f4-aecb-def3bc44179f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects:  20% (1/5)\rUnpacking objects:  40% (2/5)\rUnpacking objects:  60% (3/5)\rUnpacking objects:  80% (4/5)\rUnpacking objects: 100% (5/5)\rUnpacking objects: 100% (5/5), 460 bytes | 460.00 KiB/s, done.\n",
            "From https://github.com/annanyapr/composable-sft\n",
            " * branch            HEAD       -> FETCH_HEAD\n",
            "Updating c480346..cc6d4b2\n",
            "Fast-forward\n",
            " examples/text-classification/eval_nli.sh | 10 \u001b[32m++\u001b[m\u001b[31m--------\u001b[m\n",
            " 1 file changed, 2 insertions(+), 8 deletions(-)\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "condacolab.check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBlVuhLtDlut",
        "outputId": "7257baed-5299-4d96-b046-e72f7ca0cedc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨🍰✨ Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda env create -f composable-sft/mlenv.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neXcCXRYDqdC",
        "outputId": "7c491451-e052-4617-d99e-a8aa547f3cc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Channels:\n",
            " - pytorch\n",
            " - nvidia\n",
            " - defaults\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 23.11.0\n",
            "    latest version: 24.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "pytorch-2.0.0        | 1.41 GB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.0.0    | 62.5 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.0         | 18.1 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.11.1         | 11.7 MB   | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-11.2.0     | 5.3 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libstdcxx-ng-11.2.0  | 4.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "intel-openmp-2022.1. | 4.5 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-1.1.1t       | 3.7 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tk-8.6.12            | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-2.8.4       | 2.6 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :   0% 0.00010570613102011707/1 [00:00<19:02, 1142.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   0% 0.00010941492379625203/1 [00:00<19:25, 1165.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   0% 1.0851763213615301e-05/1 [00:00<3:43:39, 13420.02s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   0% 8.861394863273745e-05/1 [00:00<29:19, 1759.49s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :   1% 0.01405891542567557/1 [00:00<00:12, 13.17s/it]     \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   2% 0.015646334102864038/1 [00:00<00:11, 12.13s/it]    \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   0% 0.0011828421902840677/1 [00:00<02:51, 171.23s/it]     \n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   1% 0.008418325120110058/1 [00:00<00:24, 24.96s/it]   \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :   4% 0.03509443549867887/1 [00:00<00:07,  7.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   4% 0.03895171287146572/1 [00:00<00:06,  6.80s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   0% 0.003505119517997742/1 [00:00<01:16, 76.95s/it]  \n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   2% 0.0208242779286933/1 [00:00<00:13, 13.52s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :   6% 0.0619437927777886/1 [00:00<00:05,  5.38s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   7% 0.06794666767747251/1 [00:00<00:04,  4.97s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   1% 0.00630487442711049/1 [00:00<00:53, 53.93s/it] \n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   3% 0.02915398910017062/1 [00:00<00:15, 15.77s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :   9% 0.08889885618791846/1 [00:00<00:04,  4.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :   9% 0.094643909083758/1 [00:00<00:04,  4.46s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   1% 0.008931001124805393/1 [00:00<00:46, 47.15s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   5% 0.04652232303218716/1 [00:00<00:09, 10.36s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  11% 0.1108857314401028/1 [00:00<00:04,  4.67s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  12% 0.11762104308097092/1 [00:00<00:03,  4.43s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   1% 0.011144760820382913/1 [00:00<00:46, 46.53s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   7% 0.06539709409096024/1 [00:00<00:07,  8.06s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  13% 0.13350684347840786/1 [00:00<00:03,  4.59s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  14% 0.1415829113923501/1 [00:00<00:03,  4.35s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   1% 0.013358520515960435/1 [00:00<00:45, 46.48s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :   9% 0.08551246043059164/1 [00:00<00:06,  6.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  16% 0.16130755593669865/1 [00:00<00:03,  4.22s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  17% 0.16937430203659812/1 [00:00<00:03,  4.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   2% 0.016266793057209336/1 [00:00<00:41, 41.90s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  11% 0.10864070102373612/1 [00:00<00:05,  5.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  19% 0.18995391744315038/1 [00:00<00:03,  3.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  20% 0.19946340608056742/1 [00:00<00:03,  3.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   2% 0.01897973386061316/1 [00:00<00:39, 40.22s/it] \n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  13% 0.13052834633602226/1 [00:01<00:04,  5.37s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  22% 0.21532338888797847/1 [00:01<00:03,  3.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  23% 0.2278018713437967/1 [00:01<00:02,  3.72s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   2% 0.021736081716871445/1 [00:01<00:38, 38.97s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  15% 0.1540110427236977/1 [00:01<00:04,  4.98s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  24% 0.24206704003606808/1 [00:01<00:02,  3.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  25% 0.25482735752147095/1 [00:01<00:02,  3.73s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   2% 0.02456839191562504/1 [00:01<00:36, 37.81s/it] \n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  18% 0.17829126464906775/1 [00:01<00:03,  4.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  27% 0.2694449279702784/1 [00:01<00:02,  3.83s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  29% 0.285682366032014/1 [00:01<00:02,  3.57s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   3% 0.02754177503615563/1 [00:01<00:35, 36.48s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  20% 0.20496406318752172/1 [00:01<00:03,  4.36s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  30% 0.30242524084855493/1 [00:01<00:02,  3.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  32% 0.31785035362811215/1 [00:01<00:02,  3.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   3% 0.03080815576345384/1 [00:01<00:33, 34.50s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  23% 0.23057349434238283/1 [00:01<00:03,  4.22s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  33% 0.33064877783092617/1 [00:01<00:02,  3.59s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  35% 0.3471735532055077/1 [00:01<00:02,  3.43s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   3% 0.03380324241041166/1 [00:01<00:33, 34.17s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  25% 0.25449926047322197/1 [00:01<00:03,  4.40s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  36% 0.3586609025512572/1 [00:01<00:02,  3.73s/it] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  38% 0.37649675278290323/1 [00:01<00:02,  3.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   4% 0.036744070241301406/1 [00:01<00:35, 37.21s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  28% 0.2775388871177337/1 [00:01<00:03,  4.44s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  39% 0.3856159659613871/1 [00:01<00:02,  3.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  40% 0.40461638819853996/1 [00:01<00:02,  3.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   4% 0.03955467691362777/1 [00:01<00:35, 36.74s/it] \n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  30% 0.3003126719163472/1 [00:01<00:03,  4.59s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  41% 0.4124653232404968/1 [00:01<00:02,  3.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  43% 0.4326266086903805/1 [00:01<00:02,  3.66s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   4% 0.04231102476988605/1 [00:01<00:35, 36.85s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  32% 0.3222889311772661/1 [00:01<00:03,  4.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  46% 0.4616215634963873/1 [00:01<00:01,  3.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  18% 0.18022327966498106/1 [00:01<00:07,  9.11s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   5% 0.04505652086293073/1 [00:01<00:35, 36.73s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  34% 0.34391073464365407/1 [00:01<00:03,  4.67s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  49% 0.4917106675403566/1 [00:01<00:01,  3.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  19% 0.19177165084884956/1 [00:01<00:07,  8.97s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   5% 0.04791053458811155/1 [00:01<00:34, 36.25s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  37% 0.367570658928595/1 [00:02<00:02,  4.53s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  52% 0.5214715268129372/1 [00:02<00:01,  3.47s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  50% 0.4986158200218922/1 [00:02<00:01,  3.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   5% 0.05068858597079707/1 [00:02<00:34, 36.80s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  39% 0.3897241460867793/1 [00:02<00:02,  4.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  55% 0.5519982905520915/1 [00:02<00:01,  3.42s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  53% 0.5283192428385451/1 [00:02<00:01,  3.56s/it]\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   5% 0.05354259969597789/1 [00:02<00:34, 36.32s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  41% 0.4118776332449637/1 [00:02<00:02,  4.55s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  58% 0.581321490129487/1 [00:02<00:01,  3.47s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  23% 0.2250000497199061/1 [00:02<00:07,  9.24s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   6% 0.05634235460509064/1 [00:02<00:34, 36.19s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  43% 0.4339425064545153/1 [00:02<00:02,  4.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  61% 0.6102070300116975/1 [00:02<00:01,  3.66s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  24% 0.23586152893744783/1 [00:02<00:07,  9.72s/it]\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   6% 0.05912040598777615/1 [00:02<00:35, 37.59s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  46% 0.455652923869536/1 [00:02<00:02,  4.65s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  64% 0.6381078355797418/1 [00:02<00:01,  3.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  61% 0.610347200510156/1 [00:02<00:01,  3.83s/it] \u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   6% 0.06189845737046167/1 [00:02<00:34, 37.13s/it]\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  67% 0.6692910888616737/1 [00:02<00:01,  3.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  48% 0.47718611338729117/1 [00:02<00:02,  4.98s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  64% 0.6403677417198692/1 [00:02<00:01,  3.68s/it]\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   6% 0.06498035812312841/1 [00:02<00:33, 35.65s/it]\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  70% 0.6979577988962916/1 [00:02<00:01,  3.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  50% 0.501200493466763/1 [00:02<00:02,  4.73s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  67% 0.6697540461434618/1 [00:02<00:01,  3.60s/it]\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   7% 0.06793203771723177/1 [00:02<00:32, 35.22s/it]\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  73% 0.7266245089309097/1 [00:02<00:00,  3.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  53% 0.525480715392133/1 [00:02<00:02,  4.54s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   7% 0.07078605144241261/1 [00:02<00:32, 35.37s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  28% 0.2820550136729218/1 [00:02<00:06,  9.00s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  76% 0.7551818040417314/1 [00:02<00:00,  3.70s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   7% 0.0736183616411662/1 [00:02<00:34, 37.05s/it] \n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  73% 0.7255668833220835/1 [00:02<00:01,  3.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  29% 0.2932170081257315/1 [00:02<00:06,  9.43s/it]\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  57% 0.5696990757598691/1 [00:03<00:02,  4.68s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  78% 0.7824261200669982/1 [00:03<00:00,  3.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :   8% 0.07634215420778363/1 [00:03<00:34, 37.76s/it]\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  75% 0.7520991222081329/1 [00:03<00:00,  3.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  59% 0.5912322652776243/1 [00:03<00:01,  4.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  81% 0.8091233614732837/1 [00:03<00:00,  3.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   8% 0.07901168795833301/1 [00:03<00:35, 38.61s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  31% 0.3145535898297561/1 [00:03<00:06,  9.84s/it]\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  61% 0.6126768408467468/1 [00:03<00:01,  4.84s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  84% 0.8354923581081805/1 [00:03<00:00,  3.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   8% 0.08161611112960067/1 [00:03<00:35, 39.14s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  32% 0.32477110782886653/1 [00:03<00:06,  9.99s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  86% 0.8626272592096509/1 [00:03<00:00,  3.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  63% 0.63350111877544/1 [00:03<00:01,  4.93s/it]  \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   8% 0.08417712724801389/1 [00:03<00:36, 39.40s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  34% 0.3356325870464083/1 [00:03<00:06,  9.77s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  89% 0.8929351931012127/1 [00:03<00:00,  3.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  66% 0.6586674801871375/1 [00:03<00:01,  4.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   9% 0.08743265621209848/1 [00:03<00:33, 36.45s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  35% 0.34834008842345315/1 [00:03<00:05,  9.13s/it]\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  69% 0.6879986971845735/1 [00:03<00:01,  4.18s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   9% 0.09118736628400936/1 [00:03<00:29, 32.95s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  36% 0.3628936119600012/1 [00:03<00:05,  8.35s/it] \u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  72% 0.7200769465896245/1 [00:03<00:01,  3.80s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :   9% 0.09479015167092965/1 [00:03<00:28, 31.25s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  38% 0.377017828017595/1 [00:03<00:04,  7.94s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  92% 0.9205077538978682/1 [00:03<00:00,  5.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  75% 0.746572517230813/1 [00:03<00:01,  4.00s/it] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | :  97% 0.9662597436548901/1 [00:03<00:00,  3.26s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  10% 0.09801312534537339/1 [00:03<00:29, 32.43s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  39% 0.3896823986467444/1 [00:03<00:05,  8.38s/it]\u001b[A\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  10% 0.10223446123546974/1 [00:03<00:26, 29.32s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  41% 0.4062107365864819/1 [00:03<00:04,  7.56s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | :  97% 0.969416224834793/1 [00:03<00:00,  5.46s/it] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  11% 0.10584809838560363/1 [00:03<00:25, 28.83s/it]\n",
            "pytorch-2.0.0        | 1.41 GB   | :  11% 0.11030817306639953/1 [00:04<00:23, 26.60s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  83% 0.8343003263772231/1 [00:04<00:00,  3.95s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  44% 0.4377219055417215/1 [00:04<00:04,  8.23s/it]\u001b[A\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  11% 0.11409543842795127/1 [00:04<00:28, 32.68s/it]\n",
            "pytorch-2.0.0        | 1.41 GB   | :  12% 0.11760055794594901/1 [00:04<00:27, 31.51s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  48% 0.48309970606718244/1 [00:04<00:02,  5.72s/it]\u001b[A\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  12% 0.12095375277895613/1 [00:04<00:29, 33.81s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  50% 0.5013882046706322/1 [00:04<00:03,  6.21s/it] \u001b[A\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  13% 0.1256742697768788/1 [00:04<00:25, 29.30s/it] \n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  95% 0.9479920224730253/1 [00:04<00:00,  4.11s/it]\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  13% 0.13012349269446108/1 [00:04<00:23, 27.08s/it]\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | :  98% 0.9845895832583458/1 [00:04<00:00,  3.64s/it]\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  54% 0.5355610799953879/1 [00:04<00:03,  6.79s/it]\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  14% 0.13994433840278292/1 [00:04<00:22, 26.74s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   0% 0.0001619872369599508/1 [00:04<8:33:05, 30790.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :   0% 0.00012044502466443428/1 [00:05<11:33:25, 41610.88s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  15% 0.14525085061424078/1 [00:05<00:20, 24.17s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   6% 0.055237647803343226/1 [00:05<01:01, 65.08s/it]      \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :   1% 0.007588036553859359/1 [00:05<07:54, 477.82s/it]       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :   9% 0.09071285269757245/1 [00:05<00:32, 35.28s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  60% 0.5955782655531878/1 [00:05<00:03,  7.89s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  15% 0.14971092529503668/1 [00:05<00:23, 28.13s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  13% 0.12602607035484173/1 [00:05<00:19, 22.14s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :   6% 0.058415836962250625/1 [00:05<00:39, 41.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  61% 0.6095307586191999/1 [00:05<00:03,  8.76s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  15% 0.1536175600519382/1 [00:05<00:27, 32.58s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :   9% 0.08587730258574164/1 [00:05<00:22, 24.24s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  62% 0.6218948140130814/1 [00:05<00:03,  9.15s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  21% 0.20766763778265693/1 [00:05<00:07,  9.90s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  16% 0.1570250137010134/1 [00:05<00:28, 33.80s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  63% 0.6334431851969499/1 [00:05<00:03,  9.12s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  25% 0.24670656189000506/1 [00:05<00:05,  7.48s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  16% 0.16020458032260268/1 [00:05<00:30, 36.24s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  64% 0.6448627641371321/1 [00:05<00:03,  9.47s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  29% 0.2852595242864734/1 [00:05<00:04,  6.07s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  16% 0.16312370462706519/1 [00:05<00:31, 37.74s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  32% 0.32316453773510184/1 [00:05<00:03,  5.02s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  66% 0.6557242433546738/1 [00:05<00:03,  9.75s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  17% 0.16616219832687748/1 [00:05<00:30, 36.48s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  37% 0.36576718105556894/1 [00:05<00:02,  4.13s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  23% 0.226316201344472/1 [00:05<00:04,  5.87s/it]  \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  17% 0.16899450852563108/1 [00:06<00:31, 37.83s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  25% 0.253777666967963/1 [00:06<00:03,  5.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  40% 0.40448213068899713/1 [00:06<00:02,  3.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  17% 0.1717074493290349/1 [00:06<00:31, 38.46s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  28% 0.2811186875667896/1 [00:06<00:03,  4.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  44% 0.4409292590049861/1 [00:06<00:01,  3.55s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  17% 0.1745506112910021/1 [00:06<00:31, 37.56s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  31% 0.30942326836293166/1 [00:06<00:03,  4.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  48% 0.4804541448232141/1 [00:06<00:01,  3.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  70% 0.6977105147963965/1 [00:06<00:02,  9.78s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  18% 0.1772527003311923/1 [00:06<00:32, 39.11s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  52% 0.5177112093240027/1 [00:06<00:01,  3.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  71% 0.7080138942912977/1 [00:06<00:02,  9.77s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  18% 0.17995478937138254/1 [00:06<00:31, 38.54s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  56% 0.5593419292227101/1 [00:06<00:01,  2.87s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  72% 0.7183172737861989/1 [00:06<00:02,  9.77s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  18% 0.1826351748851455/1 [00:06<00:31, 38.23s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  60% 0.5978948916191784/1 [00:06<00:01,  2.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  19% 0.18558685447924886/1 [00:06<00:30, 36.89s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  43% 0.42721850248474835/1 [00:06<00:02,  3.57s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  64% 0.6375817646743663/1 [00:06<00:00,  2.71s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  74% 0.7396109247423281/1 [00:06<00:02,  9.65s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  46% 0.45660708850287035/1 [00:06<00:01,  3.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  19% 0.18832149880907992/1 [00:06<00:30, 37.85s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  75% 0.7506011962035561/1 [00:06<00:02,  9.49s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  49% 0.4870796797429722/1 [00:06<00:01,  3.47s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  19% 0.19098018079641568/1 [00:06<00:31, 38.47s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  76% 0.7611621601858299/1 [00:06<00:02,  9.65s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  52% 0.5193589463530406/1 [00:06<00:01,  3.35s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  19% 0.19367141807339228/1 [00:06<00:30, 38.13s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  77% 0.7723241546386397/1 [00:06<00:02,  9.44s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  55% 0.5498315375931424/1 [00:07<00:01,  3.38s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  81% 0.8109081082215137/1 [00:07<00:00,  2.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  20% 0.1963083965343008/1 [00:07<00:31, 39.21s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  58% 0.584399259671835/1 [00:07<00:01,  3.22s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  20% 0.19904304086413183/1 [00:07<00:30, 38.45s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  79% 0.7935319440989781/1 [00:07<00:02,  9.81s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  62% 0.6172807514052256/1 [00:07<00:01,  3.17s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  20% 0.20165831579861312/1 [00:07<00:30, 38.45s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  80% 0.8037923928459839/1 [00:07<00:01,  9.95s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  20% 0.20427359073309442/1 [00:07<00:30, 38.53s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  93% 0.9312646252827571/1 [00:07<00:00,  2.67s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  21% 0.20713845622148885/1 [00:07<00:29, 37.39s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  68% 0.6799121642307314/1 [00:07<00:01,  3.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | :  97% 0.9711134855749051/1 [00:07<00:00,  2.63s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  21% 0.21010098757880583/1 [00:07<00:28, 36.26s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  72% 0.7171296768520417/1 [00:07<00:00,  3.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | :  21% 0.2130635189361228/1 [00:07<00:27, 35.49s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | :  76% 0.7551903046460029/1 [00:07<00:00,  2.98s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  85% 0.8511450077746342/1 [00:07<00:01,  8.60s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  22% 0.2158849773716628/1 [00:07<00:28, 36.12s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  86% 0.8637237169079929/1 [00:07<00:01,  8.40s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  22% 0.2189451745979023/1 [00:07<00:27, 35.05s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  88% 0.87716104099926/1 [00:07<00:00,  8.10s/it]  \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  22% 0.22254795998482257/1 [00:07<00:25, 32.55s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  89% 0.8917145645358079/1 [00:07<00:00,  7.69s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  23% 0.22579263718569356/1 [00:08<00:24, 32.04s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  90% 0.9047655118960162/1 [00:08<00:00,  7.72s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  23% 0.2291566837819143/1 [00:08<00:24, 31.34s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.0.0    | 62.5 MB   | :   0% 0.0002498735114480242/1 [00:08<9:05:39, 32747.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  92% 0.917773528508329/1 [00:08<00:00,  8.19s/it] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.0.0    | 62.5 MB   | :   5% 0.048475461220916685/1 [00:08<01:54, 120.42s/it]     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  23% 0.2323579539299308/1 [00:08<00:27, 35.73s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  93% 0.930094653154315/1 [00:08<00:00,  8.79s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  24% 0.2352553747079661/1 [00:08<00:27, 36.43s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  94% 0.9416430243381836/1 [00:08<00:00,  8.85s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  24% 0.23806598138029245/1 [00:08<00:27, 36.37s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  95% 0.9530626032783658/1 [00:08<00:00,  8.91s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  24% 0.2413432138708043/1 [00:08<00:26, 34.52s/it] \n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  97% 0.9650402819411885/1 [00:08<00:00,  8.75s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  24% 0.24428404170169402/1 [00:08<00:25, 34.38s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  98% 0.9765457223771615/1 [00:08<00:00,  9.07s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  25% 0.24722486953258377/1 [00:08<00:26, 35.35s/it]\n",
            "libcublas-11.11.3.6  | 364.0 MB  | :  99% 0.9876647860820759/1 [00:08<00:00,  9.05s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  25% 0.2500897350209782/1 [00:08<00:26, 35.24s/it] \n",
            "libcublas-11.11.3.6  | 364.0 MB  | : 100% 0.9990414342743626/1 [00:08<00:00,  8.98s/it]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  25% 0.25365996511825767/1 [00:09<00:24, 32.80s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  26% 0.2570782705305465/1 [00:09<00:23, 31.68s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  26% 0.2624824486109269/1 [00:09<00:19, 26.18s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  28% 0.27797876647996955/1 [00:09<00:14, 20.71s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | :   0% 0.0003020940145189511/1 [00:09<8:46:20, 31589.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.0.0    | 62.5 MB   | :  79% 0.7920990312902366/1 [00:09<00:00,  2.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | :   7% 0.0722004694700293/1 [00:09<01:27, 94.03s/it]        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  28% 0.282818652873242/1 [00:09<00:18, 25.57s/it]  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | :  14% 0.14047371675131223/1 [00:09<00:35, 40.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.0.0    | 62.5 MB   | :  92% 0.9242821188462413/1 [00:09<00:00,  2.08s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | :  22% 0.2153930323520121/1 [00:09<00:17, 22.10s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  29% 0.2870182852369111/1 [00:09<00:20, 29.45s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  29% 0.29097917880988067/1 [00:09<00:20, 28.33s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  30% 0.2954609570171038/1 [00:10<00:18, 26.54s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  31% 0.30649720020535054/1 [00:10<00:15, 22.00s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  31% 0.31223778294535304/1 [00:10<00:16, 23.66s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | :  78% 0.7784962754153368/1 [00:10<00:00,  2.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  32% 0.3167629682054306/1 [00:10<00:15, 23.25s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  19% 0.19283841687746586/1 [00:10<00:31, 38.77s/it]       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  32% 0.32123389464944013/1 [00:10<00:18, 26.52s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  39% 0.3850587619059655/1 [00:10<00:10, 16.36s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | :  98% 0.9769720429542877/1 [00:10<00:00,  2.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  33% 0.32521649174883693/1 [00:10<00:19, 28.51s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | :   0% 0.0008163460135194354/1 [00:10<3:43:21, 13412.68s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  33% 0.32904716416324314/1 [00:10<00:18, 27.89s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  33% 0.33281272599836764/1 [00:11<00:18, 27.54s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | :  99% 0.9870607427990801/1 [00:11<00:00,  3.85s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  34% 0.3365457325438513/1 [00:11<00:19, 29.51s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  34% 0.34039810848468477/1 [00:11<00:18, 28.50s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  36% 0.35748963554612884/1 [00:11<00:12, 20.04s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  36% 0.3646843545567558/1 [00:11<00:11, 17.77s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.11.1         | 11.7 MB   | :   0% 0.0013374072665522719/1 [00:11<2:26:02, 8773.93s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.0         | 18.1 MB   | :  29% 0.29381764559094253/1 [00:11<00:19, 28.11s/it]       \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  37% 0.370403233770331/1 [00:11<00:12, 19.92s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.11.1         | 11.7 MB   | :  14% 0.14176517025454083/1 [00:11<00:50, 58.81s/it]      \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.0         | 18.1 MB   | :  59% 0.5928051031277667/1 [00:11<00:04, 11.65s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-11.2.0     | 5.3 MB    | :  18% 0.17839899582020155/1 [00:11<00:38, 47.10s/it]     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.11.1         | 11.7 MB   | :  43% 0.4252955107636225/1 [00:11<00:08, 15.48s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-11.2.0     | 5.3 MB    | :  73% 0.7252942780886883/1 [00:12<00:02,  8.87s/it] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.0         | 18.1 MB   | :  89% 0.8892076546916501/1 [00:12<00:00,  6.59s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  38% 0.3800070442143806/1 [00:12<00:15, 25.56s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  39% 0.39048984747873294/1 [00:12<00:13, 22.33s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "intel-openmp-2022.1. | 4.5 MB    | :   0% 0.003461395531411045/1 [00:12<59:18, 3570.79s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-1.1.1t       | 3.7 MB    | :   0% 0.004179907462400222/1 [00:12<49:13, 2965.96s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  40% 0.39528632681915093/1 [00:12<00:14, 23.77s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  41% 0.4142552089165505/1 [00:12<00:12, 20.71s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  42% 0.41940979644301773/1 [00:12<00:11, 20.39s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  43% 0.42507441684052494/1 [00:13<00:11, 19.74s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | :  43% 0.4343418226249524/1 [00:13<00:09, 16.04s/it] \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "pytorch-2.0.0        | 1.41 GB   | : 100% 0.9958880136399033/1 [00:21<00:00, 15.99s/it]\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | : 100% 1.0/1 [00:27<00:00, 218.32s/it]              \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "libnpp-11.8.0.86     | 147.8 MB  | : 100% 1.0/1 [00:27<00:00, 218.32s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | : 100% 1.0/1 [00:27<00:00, 265.39s/it]             \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libcufft-10.9.0.58   | 142.8 MB  | : 100% 1.0/1 [00:27<00:00, 265.39s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "mkl-2022.1.0         | 129.7 MB  | : 100% 1.0/1 [00:30<00:00,  2.93s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "libcusparse-11.7.5.8 | 176.3 MB  | : 100% 1.0/1 [00:32<00:00,  3.64s/it]               \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-cupti-11.8.87   | 25.3 MB   | : 100% 1.0/1 [00:36<00:00,  3.85s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "cuda-nvrtc-11.8.89   | 19.1 MB   | : 100% 1.0/1 [00:39<00:00,  4.72s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "torchtriton-2.0.0    | 62.5 MB   | : 100% 1.0/1 [00:40<00:00,  1.99s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-11.2.0     | 5.3 MB    | : 100% 1.0/1 [00:40<00:00,  8.87s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.11.1         | 11.7 MB   | : 100% 1.0/1 [00:41<00:00, 46.95s/it]              \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "sympy-1.11.1         | 11.7 MB   | : 100% 1.0/1 [00:41<00:00, 46.95s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.9.0         | 18.1 MB   | : 100% 1.0/1 [00:42<00:00,  6.59s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libstdcxx-ng-11.2.0  | 4.7 MB    | : 100% 1.0/1 [00:42<00:00, 38.74s/it]                     \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libstdcxx-ng-11.2.0  | 4.7 MB    | : 100% 1.0/1 [00:42<00:00, 38.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "intel-openmp-2022.1. | 4.5 MB    | : 100% 1.0/1 [00:42<00:00, 38.81s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "intel-openmp-2022.1. | 4.5 MB    | : 100% 1.0/1 [00:42<00:00, 38.81s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tk-8.6.12            | 3.0 MB    | : 100% 1.0/1 [00:42<00:00, 38.99s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tk-8.6.12            | 3.0 MB    | : 100% 1.0/1 [00:42<00:00, 38.99s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-1.1.1t       | 3.7 MB    | : 100% 1.0/1 [00:42<00:00, 39.06s/it]                   \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "openssl-1.1.1t       | 3.7 MB    | : 100% 1.0/1 [00:42<00:00, 39.06s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "networkx-2.8.4       | 2.6 MB    | : 100% 1.0/1 [00:45<00:00,  6.06s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcurand-10.3.2.106 | 51.7 MB   | : 100% 1.0/1 [00:45<00:00,  2.01s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libcusolver-11.4.1.4 | 96.5 MB   | : 100% 1.0/1 [00:47<00:00,  2.63s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "pytorch-2.0.0        | 1.41 GB   | : 100% 1.0/1 [03:49<00:00, 15.99s/it]               \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                         \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                         \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\u001b[A\n",
            "\n",
            "\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Installing pip dependencies: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ Ran pip subprocess with arguments:\n",
            "['/usr/local/envs/mlenv/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/composable-sft/condaenv._trus4dr.requirements.txt', '--exists-action=b']\n",
            "Pip subprocess output:\n",
            "Collecting aiohttp==3.8.4\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 1.4 MB/s eta 0:00:00\n",
            "Collecting aiosignal==1.3.1\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout==4.0.2\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting attrs==23.1.0\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 10.3 MB/s eta 0:00:00\n",
            "Collecting certifi==2022.12.7\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 kB 23.2 MB/s eta 0:00:00\n",
            "Collecting charset-normalizer==3.1.0\n",
            "  Downloading charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.2/199.2 kB 28.0 MB/s eta 0:00:00\n",
            "Collecting conllu==4.5.2\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting datasets==2.11.0\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 468.7/468.7 kB 47.8 MB/s eta 0:00:00\n",
            "Collecting dill==0.3.6\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 17.9 MB/s eta 0:00:00\n",
            "Collecting filelock==3.12.0\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting frozenlist==1.3.3\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 kB 23.0 MB/s eta 0:00:00\n",
            "Collecting fsspec==2023.4.0\n",
            "  Downloading fsspec-2023.4.0-py3-none-any.whl (153 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.0/154.0 kB 24.4 MB/s eta 0:00:00\n",
            "Collecting huggingface-hub==0.13.4\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.1/200.1 kB 31.1 MB/s eta 0:00:00\n",
            "Collecting idna==3.4\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 11.6 MB/s eta 0:00:00\n",
            "Collecting joblib==1.2.0\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 40.4 MB/s eta 0:00:00\n",
            "Collecting multidict==6.0.4\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 17.0 MB/s eta 0:00:00\n",
            "Collecting multiprocess==0.70.14\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.9/132.9 kB 21.3 MB/s eta 0:00:00\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 76.6 MB/s eta 0:00:00\n",
            "Collecting packaging==23.1\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 8.1 MB/s eta 0:00:00\n",
            "Collecting pandas==2.0.0\n",
            "  Downloading pandas-2.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.4/12.4 MB 68.9 MB/s eta 0:00:00\n",
            "Collecting pyarrow==11.0.0\n",
            "  Downloading pyarrow-11.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.9/34.9 MB 21.6 MB/s eta 0:00:00\n",
            "Collecting python-dateutil==2.8.2\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 35.2 MB/s eta 0:00:00\n",
            "Collecting pytz==2023.3\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.3/502.3 kB 57.6 MB/s eta 0:00:00\n",
            "Collecting pyyaml==6.0\n",
            "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 661.8/661.8 kB 60.8 MB/s eta 0:00:00\n",
            "Collecting regex==2023.3.23\n",
            "  Downloading regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.0/769.0 kB 58.9 MB/s eta 0:00:00\n",
            "Collecting requests==2.28.2\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 10.5 MB/s eta 0:00:00\n",
            "Collecting responses==0.18.0\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting scikit-learn==1.2.2\n",
            "  Downloading scikit_learn-1.2.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 58.9 MB/s eta 0:00:00\n",
            "Collecting scipy==1.10.1\n",
            "  Downloading scipy-1.10.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.5/34.5 MB 45.0 MB/s eta 0:00:00\n",
            "Collecting seqeval==1.2.2\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 6.6 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting six==1.16.0\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting threadpoolctl==3.1.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting tokenizers==0.13.3\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 101.5 MB/s eta 0:00:00\n",
            "Collecting tqdm==4.65.0\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 13.7 MB/s eta 0:00:00\n",
            "Collecting transformers==4.28.1\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 100.6 MB/s eta 0:00:00\n",
            "Collecting tzdata==2023.3\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 45.5 MB/s eta 0:00:00\n",
            "Collecting urllib3==1.26.15\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.9/140.9 kB 22.8 MB/s eta 0:00:00\n",
            "Collecting xxhash==3.2.0\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.2/212.2 kB 30.8 MB/s eta 0:00:00\n",
            "Collecting yarl==1.9.1\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 269.3/269.3 kB 38.7 MB/s eta 0:00:00\n",
            "Collecting fsspec[http]>=2021.11.1\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.6/179.6 kB 32.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/envs/mlenv/lib/python3.9/site-packages (from huggingface-hub==0.13.4->-r /content/composable-sft/condaenv._trus4dr.requirements.txt (line 13)) (4.5.0)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 179.3/179.3 kB 10.5 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.6/177.6 kB 29.4 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.9/176.9 kB 29.0 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.1/316.1 kB 42.3 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 26.7 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2024.3.0-py3-none-any.whl (171 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171.9/171.9 kB 28.3 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 170.9/170.9 kB 27.2 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 169.0/169.0 kB 28.7 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.9/168.9 kB 28.5 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.12.0-py3-none-any.whl (168 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 168.9/168.9 kB 30.0 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 28.3 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 31.8 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.4/173.4 kB 28.6 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 173.2/173.2 kB 29.2 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.8/163.8 kB 26.7 MB/s eta 0:00:00\n",
            "  Downloading fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 160.1/160.1 kB 26.8 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16164 sha256=a1d5137ca416958f1f2a48b09948b3160b7d16e6067f3fddb94ce0b25b7e3a81\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tokenizers, pytz, xxhash, urllib3, tzdata, tqdm, threadpoolctl, six, regex, pyyaml, packaging, numpy, multidict, joblib, idna, fsspec, frozenlist, filelock, dill, conllu, charset-normalizer, certifi, attrs, async-timeout, yarl, scipy, requests, python-dateutil, pyarrow, multiprocess, aiosignal, scikit-learn, responses, pandas, huggingface-hub, aiohttp, transformers, seqeval, datasets\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2022.12.7 charset-normalizer-3.1.0 conllu-4.5.2 datasets-2.11.0 dill-0.3.6 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.4.0 huggingface-hub-0.13.4 idna-3.4 joblib-1.2.0 multidict-6.0.4 multiprocess-0.70.14 numpy-1.24.3 packaging-23.1 pandas-2.0.0 pyarrow-11.0.0 python-dateutil-2.8.2 pytz-2023.3 pyyaml-6.0 regex-2023.3.23 requests-2.28.2 responses-0.18.0 scikit-learn-1.2.2 scipy-1.10.1 seqeval-1.2.2 six-1.16.0 threadpoolctl-3.1.0 tokenizers-0.13.3 tqdm-4.65.0 transformers-4.28.1 tzdata-2023.3 urllib3-1.26.15 xxhash-3.2.0 yarl-1.9.1\n",
            "\n",
            "\b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate mlenv\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('composable-sft/examples/text-classification')"
      ],
      "metadata": {
        "id": "EMaot0BQhZBQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the AmericasNLI dataset"
      ],
      "metadata": {
        "id": "M9ZHKlJB8Ctp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd data && git clone https://github.com/nala-cub/AmericasNLI.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XXQHbVxAxZT",
        "outputId": "895d35bb-2b4b-4b46-b4c2-c331271c9773"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AmericasNLI'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 41 (delta 28), reused 24 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (41/41), 504.50 KiB | 1.86 MiB/s, done.\n",
            "Resolving deltas: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Sentiment Analysis Model. **Run this only if you want to train the model**"
      ],
      "metadata": {
        "id": "8kjsCBhJzV0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
        "!source activate mlenv && ./train_sa.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLEmJi7d8Ro1",
        "outputId": "1f58c427-4857-4c3c-a527-934dd1a53d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
            "11/12/2024 18:08:43 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-8cb4a4226acc80f0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 9500.12it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1428.58it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-8cb4a4226acc80f0/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 845.63it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 18:08:47,323 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 18:08:47,323 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.48k/1.48k [00:00<00:00, 197kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.23it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:01, 36.0MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 98.8MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.97it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "Downloading builder script: 6.50kB [00:00, 4.69MB/s]       \n",
            "/usr/local/envs/mlenv/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "{'loss': 0.4356, 'learning_rate': 1.8492840994724945e-05, 'epoch': 0.38}\n",
            "{'loss': 0.3321, 'learning_rate': 1.698568198944989e-05, 'epoch': 0.75}\n",
            " 20% 1327/6635 [02:00<08:08, 10.87it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 84.46it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 79.31it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 78.10it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 77.03it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 76.76it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 76.78it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 76.29it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 76.56it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 76.30it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 76.07it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 76.20it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 76.30it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 76.37it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 76.20it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 75.70it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 75.32it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 74.10it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 74.46it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 74.91it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:02, 75.24it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 75.49it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.96it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 75.91it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 75.82it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 76.05it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 76.23it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 75.94it/s]\u001b[A\n",
            " 72% 226/313 [00:02<00:01, 75.99it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 75.73it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 75.87it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 76.35it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 76.51it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 76.53it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 76.07it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 76.30it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 76.64it/s]\u001b[A\n",
            " 95% 298/313 [00:03<00:00, 76.85it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.35495713353157043, 'eval_f1': 0.8629026736086578, 'eval_runtime': 4.1415, 'eval_samples_per_second': 302.061, 'eval_steps_per_second': 75.576, 'epoch': 1.0}\n",
            " 20% 1327/6635 [02:04<08:08, 10.87it/s]\n",
            "100% 313/313 [00:04<00:00, 76.68it/s]\u001b[A\n",
            "{'loss': 0.3028, 'learning_rate': 1.547852298417483e-05, 'epoch': 1.13}\n",
            "{'loss': 0.2424, 'learning_rate': 1.3971363978899775e-05, 'epoch': 1.51}\n",
            "{'loss': 0.2423, 'learning_rate': 1.246420497362472e-05, 'epoch': 1.88}\n",
            " 40% 2655/6635 [04:16<06:00, 11.03it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 87.26it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 80.91it/s]\u001b[A\n",
            "  9% 27/313 [00:00<00:03, 78.89it/s]\u001b[A\n",
            " 11% 35/313 [00:00<00:03, 78.15it/s]\u001b[A\n",
            " 14% 43/313 [00:00<00:03, 77.47it/s]\u001b[A\n",
            " 16% 51/313 [00:00<00:03, 77.13it/s]\u001b[A\n",
            " 19% 59/313 [00:00<00:03, 77.18it/s]\u001b[A\n",
            " 21% 67/313 [00:00<00:03, 77.04it/s]\u001b[A\n",
            " 24% 75/313 [00:00<00:03, 77.24it/s]\u001b[A\n",
            " 27% 83/313 [00:01<00:02, 77.03it/s]\u001b[A\n",
            " 29% 91/313 [00:01<00:02, 77.07it/s]\u001b[A\n",
            " 32% 99/313 [00:01<00:02, 77.38it/s]\u001b[A\n",
            " 34% 107/313 [00:01<00:02, 77.21it/s]\u001b[A\n",
            " 37% 115/313 [00:01<00:02, 77.11it/s]\u001b[A\n",
            " 39% 123/313 [00:01<00:02, 77.25it/s]\u001b[A\n",
            " 42% 131/313 [00:01<00:02, 77.07it/s]\u001b[A\n",
            " 44% 139/313 [00:01<00:02, 77.09it/s]\u001b[A\n",
            " 47% 147/313 [00:01<00:02, 77.05it/s]\u001b[A\n",
            " 50% 155/313 [00:01<00:02, 77.05it/s]\u001b[A\n",
            " 52% 163/313 [00:02<00:01, 77.10it/s]\u001b[A\n",
            " 55% 171/313 [00:02<00:01, 76.86it/s]\u001b[A\n",
            " 57% 179/313 [00:02<00:01, 76.85it/s]\u001b[A\n",
            " 60% 187/313 [00:02<00:01, 76.94it/s]\u001b[A\n",
            " 62% 195/313 [00:02<00:01, 77.04it/s]\u001b[A\n",
            " 65% 203/313 [00:02<00:01, 76.99it/s]\u001b[A\n",
            " 67% 211/313 [00:02<00:01, 76.13it/s]\u001b[A\n",
            " 70% 219/313 [00:02<00:01, 76.22it/s]\u001b[A\n",
            " 73% 227/313 [00:02<00:01, 76.56it/s]\u001b[A\n",
            " 75% 235/313 [00:03<00:01, 76.79it/s]\u001b[A\n",
            " 78% 243/313 [00:03<00:00, 75.91it/s]\u001b[A\n",
            " 80% 251/313 [00:03<00:00, 75.83it/s]\u001b[A\n",
            " 83% 259/313 [00:03<00:00, 75.89it/s]\u001b[A\n",
            " 85% 267/313 [00:03<00:00, 75.87it/s]\u001b[A\n",
            " 88% 275/313 [00:03<00:00, 75.72it/s]\u001b[A\n",
            " 90% 283/313 [00:03<00:00, 76.19it/s]\u001b[A\n",
            " 93% 291/313 [00:03<00:00, 76.15it/s]\u001b[A\n",
            " 96% 299/313 [00:03<00:00, 76.53it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.32821163535118103, 'eval_f1': 0.8988000518269316, 'eval_runtime': 4.0958, 'eval_samples_per_second': 305.436, 'eval_steps_per_second': 76.42, 'epoch': 2.0}\n",
            " 40% 2655/6635 [04:20<06:00, 11.03it/s]\n",
            "100% 313/313 [00:04<00:00, 76.76it/s]\u001b[A\n",
            "{'loss': 0.1837, 'learning_rate': 1.0957045968349663e-05, 'epoch': 2.26}\n",
            "{'loss': 0.158, 'learning_rate': 9.449886963074605e-06, 'epoch': 2.64}\n",
            " 60% 3981/6635 [06:31<04:00, 11.02it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 84.85it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 74.76it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 75.05it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 75.28it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 75.48it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 75.73it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 76.02it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 76.13it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 75.87it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 75.45it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 75.76it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 76.01it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 75.91it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 76.10it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 76.06it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 76.36it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 76.42it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 76.31it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 76.21it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:02, 75.43it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 75.85it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.79it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 76.03it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 76.27it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 76.61it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 76.90it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 75.67it/s]\u001b[A\n",
            " 72% 226/313 [00:02<00:01, 76.11it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 75.49it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 75.88it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 76.19it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 76.53it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 76.68it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 76.79it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 76.83it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 76.82it/s]\u001b[A\n",
            " 95% 298/313 [00:03<00:00, 76.13it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.34380534291267395, 'eval_f1': 0.9130313290992086, 'eval_runtime': 4.1453, 'eval_samples_per_second': 301.791, 'eval_steps_per_second': 75.508, 'epoch': 3.0}\n",
            " 60% 3982/6635 [06:35<04:00, 11.02it/s]\n",
            "100% 313/313 [00:04<00:00, 75.09it/s]\u001b[A\n",
            "{'loss': 0.1578, 'learning_rate': 7.942727957799549e-06, 'epoch': 3.01}\n",
            "{'loss': 0.1081, 'learning_rate': 6.435568952524491e-06, 'epoch': 3.39}\n",
            "{'loss': 0.0984, 'learning_rate': 4.928409947249435e-06, 'epoch': 3.77}\n",
            " 80% 5310/6635 [08:48<01:58, 11.16it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 85.97it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 78.93it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 77.96it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 77.42it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 74.07it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 75.09it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 75.33it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 75.63it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 75.95it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 75.94it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 76.05it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 75.52it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 75.90it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 74.08it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 73.46it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 74.33it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 74.89it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 75.11it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 75.42it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:01, 75.73it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 76.09it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.58it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 75.45it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 75.69it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 75.99it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 76.00it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 75.87it/s]\u001b[A\n",
            " 72% 226/313 [00:02<00:01, 76.10it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 76.26it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 76.26it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 74.72it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 75.13it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 75.39it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 75.71it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 75.86it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 76.10it/s]\u001b[A\n",
            " 95% 298/313 [00:03<00:00, 76.40it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.42259395122528076, 'eval_f1': 0.9053961225114034, 'eval_runtime': 4.1553, 'eval_samples_per_second': 301.065, 'eval_steps_per_second': 75.326, 'epoch': 4.0}\n",
            " 80% 5310/6635 [08:53<01:58, 11.16it/s]\n",
            "100% 313/313 [00:04<00:00, 76.65it/s]\u001b[A\n",
            "{'loss': 0.0706, 'learning_rate': 3.4212509419743784e-06, 'epoch': 4.14}\n",
            "{'loss': 0.0563, 'learning_rate': 1.914091936699322e-06, 'epoch': 4.52}\n",
            "{'loss': 0.0674, 'learning_rate': 4.069329314242653e-07, 'epoch': 4.9}\n",
            "100% 6634/6635 [11:08<00:00, 10.92it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 84.88it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 79.55it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 77.31it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 76.82it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 75.98it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 76.13it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 75.51it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 75.52it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 75.69it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 75.42it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 75.76it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 75.97it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 76.03it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 76.19it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 76.27it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 76.48it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 76.42it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 76.49it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 76.26it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:02, 74.58it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 75.29it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.74it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 75.83it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 76.26it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 76.26it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 76.43it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 76.41it/s]\u001b[A\n",
            " 72% 226/313 [00:02<00:01, 76.56it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 76.45it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 76.84it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 77.09it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 77.33it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 77.33it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 77.54it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 77.61it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 77.67it/s]\u001b[A\n",
            " 95% 298/313 [00:03<00:00, 77.70it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.4199236333370209, 'eval_f1': 0.9016397693550192, 'eval_runtime': 4.1153, 'eval_samples_per_second': 303.988, 'eval_steps_per_second': 76.058, 'epoch': 5.0}\n",
            "100% 6635/6635 [11:12<00:00, 10.92it/s]\n",
            "100% 313/313 [00:04<00:00, 77.53it/s]\u001b[A\n",
            "{'train_runtime': 686.8908, 'train_samples_per_second': 77.283, 'train_steps_per_second': 9.659, 'train_loss': 0.18621188065299615, 'epoch': 5.0}\n",
            "100% 6635/6635 [11:26<00:00,  9.66it/s]\n",
            "Finding masking threshold: 100% 201/201 [00:13<00:00, 14.80it/s]\n",
            "Updating masks: 100% 201/201 [00:00<00:00, 735.44it/s]\n",
            "/usr/local/envs/mlenv/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "{'loss': 0.4197, 'learning_rate': 1.8492840994724945e-05, 'epoch': 0.38}\n",
            "{'loss': 0.2626, 'learning_rate': 1.698568198944989e-05, 'epoch': 0.75}\n",
            " 20% 1327/6635 [02:07<08:35, 10.30it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 83.18it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 77.16it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 75.26it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 74.82it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 75.19it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 75.39it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 75.40it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 75.04it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 75.26it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 75.08it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 74.91it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 75.18it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 75.22it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 75.51it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 75.36it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 75.20it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 74.72it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 74.33it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 74.82it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:02, 74.64it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 74.56it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.11it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 75.35it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 75.32it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 75.65it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 75.60it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 75.67it/s]\u001b[A\n",
            " 72% 226/313 [00:02<00:01, 76.06it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 76.40it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 76.04it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 76.34it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 76.59it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 76.51it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 76.40it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 76.55it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 75.30it/s]\u001b[A\n",
            " 95% 298/313 [00:03<00:00, 74.65it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.24473387002944946, 'eval_f1': 0.9029391744908987, 'eval_runtime': 4.1753, 'eval_samples_per_second': 299.623, 'eval_steps_per_second': 74.966, 'epoch': 1.0}\n",
            " 20% 1327/6635 [02:11<08:35, 10.30it/s]\n",
            "100% 313/313 [00:04<00:00, 75.16it/s]\u001b[A\n",
            "{'loss': 0.1967, 'learning_rate': 1.547852298417483e-05, 'epoch': 1.13}\n",
            "{'loss': 0.2018, 'learning_rate': 1.3971363978899775e-05, 'epoch': 1.51}\n",
            "{'loss': 0.1785, 'learning_rate': 1.246420497362472e-05, 'epoch': 1.88}\n",
            " 40% 2655/6635 [04:33<06:17, 10.54it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 86.53it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 80.72it/s]\u001b[A\n",
            "  9% 27/313 [00:00<00:03, 77.72it/s]\u001b[A\n",
            " 11% 35/313 [00:00<00:03, 76.81it/s]\u001b[A\n",
            " 14% 43/313 [00:00<00:03, 76.61it/s]\u001b[A\n",
            " 16% 51/313 [00:00<00:03, 76.23it/s]\u001b[A\n",
            " 19% 59/313 [00:00<00:03, 75.98it/s]\u001b[A\n",
            " 21% 67/313 [00:00<00:03, 75.97it/s]\u001b[A\n",
            " 24% 75/313 [00:00<00:03, 76.34it/s]\u001b[A\n",
            " 27% 83/313 [00:01<00:03, 76.55it/s]\u001b[A\n",
            " 29% 91/313 [00:01<00:02, 76.41it/s]\u001b[A\n",
            " 32% 99/313 [00:01<00:02, 76.17it/s]\u001b[A\n",
            " 34% 107/313 [00:01<00:02, 76.27it/s]\u001b[A\n",
            " 37% 115/313 [00:01<00:02, 76.50it/s]\u001b[A\n",
            " 39% 123/313 [00:01<00:02, 76.34it/s]\u001b[A\n",
            " 42% 131/313 [00:01<00:02, 76.29it/s]\u001b[A\n",
            " 44% 139/313 [00:01<00:02, 76.21it/s]\u001b[A\n",
            " 47% 147/313 [00:01<00:02, 76.32it/s]\u001b[A\n",
            " 50% 155/313 [00:02<00:02, 76.36it/s]\u001b[A\n",
            " 52% 163/313 [00:02<00:01, 76.58it/s]\u001b[A\n",
            " 55% 171/313 [00:02<00:01, 74.80it/s]\u001b[A\n",
            " 57% 179/313 [00:02<00:01, 72.80it/s]\u001b[A\n",
            " 60% 187/313 [00:02<00:01, 74.18it/s]\u001b[A\n",
            " 62% 195/313 [00:02<00:01, 75.16it/s]\u001b[A\n",
            " 65% 203/313 [00:02<00:01, 75.80it/s]\u001b[A\n",
            " 67% 211/313 [00:02<00:01, 76.22it/s]\u001b[A\n",
            " 70% 219/313 [00:02<00:01, 76.57it/s]\u001b[A\n",
            " 73% 227/313 [00:02<00:01, 76.85it/s]\u001b[A\n",
            " 75% 235/313 [00:03<00:01, 76.94it/s]\u001b[A\n",
            " 78% 243/313 [00:03<00:00, 74.30it/s]\u001b[A\n",
            " 80% 251/313 [00:03<00:00, 74.89it/s]\u001b[A\n",
            " 83% 259/313 [00:03<00:00, 75.40it/s]\u001b[A\n",
            " 85% 267/313 [00:03<00:00, 75.38it/s]\u001b[A\n",
            " 88% 275/313 [00:03<00:00, 75.69it/s]\u001b[A\n",
            " 90% 283/313 [00:03<00:00, 75.75it/s]\u001b[A\n",
            " 93% 291/313 [00:03<00:00, 76.10it/s]\u001b[A\n",
            " 96% 299/313 [00:03<00:00, 76.20it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.25828444957733154, 'eval_f1': 0.9158787749432933, 'eval_runtime': 4.1412, 'eval_samples_per_second': 302.086, 'eval_steps_per_second': 75.582, 'epoch': 2.0}\n",
            " 40% 2655/6635 [04:37<06:17, 10.54it/s]\n",
            "100% 313/313 [00:04<00:00, 76.17it/s]\u001b[A\n",
            "{'loss': 0.1708, 'learning_rate': 1.0957045968349663e-05, 'epoch': 2.26}\n",
            "{'loss': 0.1233, 'learning_rate': 9.449886963074605e-06, 'epoch': 2.64}\n",
            " 60% 3981/6635 [06:57<04:15, 10.40it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 84.67it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 78.78it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 77.24it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 76.54it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 76.13it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 75.36it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 74.41it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 74.92it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 75.04it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 75.30it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 75.38it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 75.40it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 75.55it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 75.48it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 74.57it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 74.37it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 74.80it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 74.88it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 75.11it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:02, 75.30it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 75.30it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.36it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 73.95it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 73.61it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 73.50it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 72.29it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 72.97it/s]\u001b[A\n",
            " 72% 226/313 [00:03<00:01, 73.51it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 73.49it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 73.34it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 73.51it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 73.57it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 73.91it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 73.29it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 72.45it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 72.55it/s]\u001b[A\n",
            " 95% 298/313 [00:04<00:00, 72.39it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.27001112699508667, 'eval_f1': 0.9174894884158215, 'eval_runtime': 4.2396, 'eval_samples_per_second': 295.073, 'eval_steps_per_second': 73.827, 'epoch': 3.0}\n",
            " 60% 3982/6635 [07:02<04:14, 10.40it/s]\n",
            "100% 313/313 [00:04<00:00, 72.35it/s]\u001b[A\n",
            "{'loss': 0.1451, 'learning_rate': 7.942727957799549e-06, 'epoch': 3.01}\n",
            "{'loss': 0.1157, 'learning_rate': 6.435568952524491e-06, 'epoch': 3.39}\n",
            "{'loss': 0.1133, 'learning_rate': 4.928409947249435e-06, 'epoch': 3.77}\n",
            " 80% 5310/6635 [12:25<02:05, 10.56it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 86.22it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 78.98it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 76.75it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 76.40it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 76.23it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 76.26it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 76.12it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 75.93it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 75.86it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 75.78it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 76.03it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 76.02it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 76.07it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 75.71it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 75.99it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 76.01it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 75.78it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 75.83it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 75.80it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:01, 75.76it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 76.03it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.95it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 76.12it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 76.27it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 76.50it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 76.46it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 75.95it/s]\u001b[A\n",
            " 72% 226/313 [00:02<00:01, 75.88it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 75.76it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 75.48it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 75.59it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 75.24it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 75.35it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 75.63it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 75.87it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 72.90it/s]\u001b[A\n",
            " 95% 298/313 [00:03<00:00, 74.00it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.27635714411735535, 'eval_f1': 0.9192782862641904, 'eval_runtime': 4.1543, 'eval_samples_per_second': 301.132, 'eval_steps_per_second': 75.343, 'epoch': 4.0}\n",
            " 80% 5310/6635 [12:29<02:05, 10.56it/s]\n",
            "100% 313/313 [00:04<00:00, 74.68it/s]\u001b[A\n",
            "{'loss': 0.1402, 'learning_rate': 3.4212509419743784e-06, 'epoch': 4.14}\n",
            "{'loss': 0.1018, 'learning_rate': 1.914091936699322e-06, 'epoch': 4.52}\n",
            "{'loss': 0.0778, 'learning_rate': 4.069329314242653e-07, 'epoch': 4.9}\n",
            "100% 6634/6635 [14:53<00:00, 10.22it/s]\n",
            "  0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 9/313 [00:00<00:03, 83.55it/s]\u001b[A\n",
            "  6% 18/313 [00:00<00:03, 77.46it/s]\u001b[A\n",
            "  8% 26/313 [00:00<00:03, 74.94it/s]\u001b[A\n",
            " 11% 34/313 [00:00<00:03, 75.66it/s]\u001b[A\n",
            " 13% 42/313 [00:00<00:03, 75.67it/s]\u001b[A\n",
            " 16% 50/313 [00:00<00:03, 75.61it/s]\u001b[A\n",
            " 19% 58/313 [00:00<00:03, 76.05it/s]\u001b[A\n",
            " 21% 66/313 [00:00<00:03, 75.90it/s]\u001b[A\n",
            " 24% 74/313 [00:00<00:03, 76.01it/s]\u001b[A\n",
            " 26% 82/313 [00:01<00:03, 75.58it/s]\u001b[A\n",
            " 29% 90/313 [00:01<00:02, 75.48it/s]\u001b[A\n",
            " 31% 98/313 [00:01<00:02, 74.56it/s]\u001b[A\n",
            " 34% 106/313 [00:01<00:02, 74.90it/s]\u001b[A\n",
            " 36% 114/313 [00:01<00:02, 75.23it/s]\u001b[A\n",
            " 39% 122/313 [00:01<00:02, 75.46it/s]\u001b[A\n",
            " 42% 130/313 [00:01<00:02, 75.36it/s]\u001b[A\n",
            " 44% 138/313 [00:01<00:02, 75.62it/s]\u001b[A\n",
            " 47% 146/313 [00:01<00:02, 75.60it/s]\u001b[A\n",
            " 49% 154/313 [00:02<00:02, 75.87it/s]\u001b[A\n",
            " 52% 162/313 [00:02<00:01, 75.65it/s]\u001b[A\n",
            " 54% 170/313 [00:02<00:01, 75.80it/s]\u001b[A\n",
            " 57% 178/313 [00:02<00:01, 75.92it/s]\u001b[A\n",
            " 59% 186/313 [00:02<00:01, 75.72it/s]\u001b[A\n",
            " 62% 194/313 [00:02<00:01, 76.10it/s]\u001b[A\n",
            " 65% 202/313 [00:02<00:01, 75.82it/s]\u001b[A\n",
            " 67% 210/313 [00:02<00:01, 76.03it/s]\u001b[A\n",
            " 70% 218/313 [00:02<00:01, 75.70it/s]\u001b[A\n",
            " 72% 226/313 [00:02<00:01, 76.06it/s]\u001b[A\n",
            " 75% 234/313 [00:03<00:01, 75.31it/s]\u001b[A\n",
            " 77% 242/313 [00:03<00:00, 74.57it/s]\u001b[A\n",
            " 80% 250/313 [00:03<00:00, 75.03it/s]\u001b[A\n",
            " 82% 258/313 [00:03<00:00, 75.06it/s]\u001b[A\n",
            " 85% 266/313 [00:03<00:00, 75.23it/s]\u001b[A\n",
            " 88% 274/313 [00:03<00:00, 75.68it/s]\u001b[A\n",
            " 90% 282/313 [00:03<00:00, 75.87it/s]\u001b[A\n",
            " 93% 290/313 [00:03<00:00, 76.05it/s]\u001b[A\n",
            " 95% 298/313 [00:03<00:00, 75.88it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 0.28291603922843933, 'eval_f1': 0.9184801965892113, 'eval_runtime': 4.1618, 'eval_samples_per_second': 300.59, 'eval_steps_per_second': 75.208, 'epoch': 5.0}\n",
            "100% 6635/6635 [14:57<00:00, 10.22it/s]\n",
            "100% 313/313 [00:04<00:00, 76.09it/s]\u001b[A\n",
            "{'train_runtime': 909.0118, 'train_samples_per_second': 58.399, 'train_steps_per_second': 7.299, 'train_loss': 0.17127179502991688, 'epoch': 5.0}\n",
            "100% 6635/6635 [15:09<00:00,  7.30it/s]\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     0.1713\n",
            "  train_runtime            = 0:15:09.01\n",
            "  train_samples            =      10617\n",
            "  train_samples_per_second =     58.399\n",
            "  train_steps_per_second   =      7.299\n",
            "100% 313/313 [00:04<00:00, 75.70it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_f1                 =     0.9193\n",
            "  eval_loss               =     0.2764\n",
            "  eval_runtime            = 0:00:04.14\n",
            "  eval_samples            =       1251\n",
            "  eval_samples_per_second =    301.453\n",
            "  eval_steps_per_second   =     75.423\n",
            "100% 313/313 [00:04<00:00, 76.27it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some commands to move the checkpoints to a different folder"
      ],
      "metadata": {
        "id": "FjpPIa5Wzi0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models/sa/checkpoints\n",
        "!mv models/sa/smsa-trimmed/checkpoint-5310 models/sa/checkpoints/\n",
        "!mv models/sa/smsa-trimmed/checkpoint-6635 models/sa/checkpoints/\n",
        "!zip -r models/sa/smsa-trimmed.zip models/sa/smsa-trimmed/"
      ],
      "metadata": {
        "id": "-QPwcu28qw13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of sentiment analysis"
      ],
      "metadata": {
        "id": "8oETSwUrzwID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running SA evaluation for languages using the author provided models."
      ],
      "metadata": {
        "id": "yfenJ37CAcnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set PYTHONPATH for the src directory\n",
        "%env PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
        "\n",
        "# List of language codes to evaluate for sentiment analysis, excluding \"bbc\", \"bug\", and \"nij\"\n",
        "languages = [\"ace\", \"ban\", \"bjn\", \"eng\", \"ind\", \"mad\", \"min\", \"jav\", \"sun\"]\n",
        "\n",
        "# Loop through each language code and run eval_sa.sh with that language\n",
        "for lang in languages:\n",
        "    print(f\"Running sentiment analysis evaluation for language: {lang}\")\n",
        "    # Run the script with the language code as an argument\n",
        "    !source activate mlenv && ./eval_sa.sh {lang}\n",
        "    print(f\"Finished evaluation for language: {lang}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_k_QsCT0Ma3",
        "outputId": "4a2b5c21-014f-4bb2-fe38-cf0976009ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
            "Running sentiment analysis evaluation for language: ace\n",
            "11/12/2024 19:19:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:19:27 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ace/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 810.70it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:19:31,177 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:19:31,177 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 39383.14it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 7339.11it/s]\n",
            "11/12/2024 19:19:38 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ace/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-d5e456596ff6a18f.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 57.47it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.7972\n",
            "  eval_loss               =      0.651\n",
            "  eval_runtime            = 0:00:00.98\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    404.209\n",
            "  eval_steps_per_second   =     50.526\n",
            "100% 50/50 [00:00<00:00, 60.30it/s]\n",
            "Finished evaluation for language: ace\n",
            "Running sentiment analysis evaluation for language: ban\n",
            "11/12/2024 19:19:45 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/ban to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ban/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 83.9kB [00:00, 8.79MB/s]       \n",
            "Downloading data: 16.6kB [00:00, 12.3MB/s]       \n",
            "Downloading data: 67.3kB [00:00, 15.1MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ban/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 909.04it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:19:51,968 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:19:51,968 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 32017.59it/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.48k/1.48k [00:00<00:00, 205kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.18it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 56.3MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 42.1MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 53.1MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.28it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 56.44it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8203\n",
            "  eval_loss               =     0.5374\n",
            "  eval_runtime            = 0:00:01.01\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    395.634\n",
            "  eval_steps_per_second   =     49.454\n",
            "100% 50/50 [00:00<00:00, 59.93it/s]\n",
            "Finished evaluation for language: ban\n",
            "Running sentiment analysis evaluation for language: bjn\n",
            "11/12/2024 19:20:08 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/bjn to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/bjn/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 83.9kB [00:00, 8.76MB/s]       \n",
            "Downloading data: 16.5kB [00:00, 11.4MB/s]       \n",
            "Downloading data: 67.0kB [00:00, 14.7MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/bjn/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 896.73it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:20:14,482 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:20:14,483 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 39383.14it/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.48k/1.48k [00:00<00:00, 206kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  4.81it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:01, 38.2MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 55.6MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 70.0MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 64.9MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.49it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 58.26it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8216\n",
            "  eval_loss               =     0.6499\n",
            "  eval_runtime            = 0:00:00.97\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    408.676\n",
            "  eval_steps_per_second   =     51.085\n",
            "100% 50/50 [00:00<00:00, 60.05it/s]\n",
            "Finished evaluation for language: bjn\n",
            "Running sentiment analysis evaluation for language: eng\n",
            "11/12/2024 19:20:30 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/eng to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/eng/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 88.4kB [00:00, 9.05MB/s]       \n",
            "Downloading data: 17.1kB [00:00, 12.0MB/s]       \n",
            "Downloading data: 69.9kB [00:00, 19.7MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/eng/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 870.97it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:20:35,981 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:20:35,981 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 33026.02it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 259, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-eng-small/revision/main\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 506, in <module>\n",
            "    main()\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 367, in main\n",
            "    lang_ft = SFT(sft_args.lang_ft)\n",
            "  File \"/content/composable-sft/src/sft/sft.py\", line 79, in __init__\n",
            "    sft_dir = pull_from_hf_model_hub(\n",
            "  File \"/content/composable-sft/src/sft/hf_utils.py\", line 5, in pull_from_hf_model_hub\n",
            "    return snapshot_download(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py\", line 183, in snapshot_download\n",
            "    repo_info = _api.repo_info(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1814, in repo_info\n",
            "    return method(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1624, in model_info\n",
            "    hf_raise_for_status(r)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 291, in hf_raise_for_status\n",
            "    raise RepositoryNotFoundError(message, response) from e\n",
            "huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6733aa88-0b7fc1d21c049c807a90e1d4)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-eng-small/revision/main.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "Invalid username or password.\n",
            "Finished evaluation for language: eng\n",
            "Running sentiment analysis evaluation for language: ind\n",
            "11/12/2024 19:20:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/ind to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ind/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 84.1kB [00:00, 8.76MB/s]       \n",
            "Downloading data: 16.6kB [00:00, 10.5MB/s]       \n",
            "Downloading data: 67.6kB [00:00, 15.8MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ind/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 879.74it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:20:50,454 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:20:50,454 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6418.22it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 259, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-ind-small/revision/main\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 506, in <module>\n",
            "    main()\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 367, in main\n",
            "    lang_ft = SFT(sft_args.lang_ft)\n",
            "  File \"/content/composable-sft/src/sft/sft.py\", line 79, in __init__\n",
            "    sft_dir = pull_from_hf_model_hub(\n",
            "  File \"/content/composable-sft/src/sft/hf_utils.py\", line 5, in pull_from_hf_model_hub\n",
            "    return snapshot_download(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py\", line 183, in snapshot_download\n",
            "    repo_info = _api.repo_info(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1814, in repo_info\n",
            "    return method(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1624, in model_info\n",
            "    hf_raise_for_status(r)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 291, in hf_raise_for_status\n",
            "    raise RepositoryNotFoundError(message, response) from e\n",
            "huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6733aa97-178a3f1b20af770138ac8004)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-ind-small/revision/main.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "Invalid username or password.\n",
            "Finished evaluation for language: ind\n",
            "Running sentiment analysis evaluation for language: mad\n",
            "11/12/2024 19:20:59 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/mad to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/mad/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 84.5kB [00:00, 8.58MB/s]       \n",
            "Downloading data: 16.8kB [00:00, 11.2MB/s]       \n",
            "Downloading data: 68.0kB [00:00, 15.6MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/mad/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 861.02it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:21:04,960 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:21:04,960 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 35394.97it/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.48k/1.48k [00:00<00:00, 202kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  4.39it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 63.2MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 72.2MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.58it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 57.40it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.7671\n",
            "  eval_loss               =     0.7215\n",
            "  eval_runtime            = 0:00:01.00\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    398.949\n",
            "  eval_steps_per_second   =     49.869\n",
            "100% 50/50 [00:00<00:00, 60.25it/s]\n",
            "Finished evaluation for language: mad\n",
            "Running sentiment analysis evaluation for language: min\n",
            "11/12/2024 19:21:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/min to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/min/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 81.5kB [00:00, 8.18MB/s]       \n",
            "Downloading data: 16.2kB [00:00, 11.8MB/s]       \n",
            "Downloading data: 65.8kB [00:00, 18.2MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/min/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 855.75it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:21:26,149 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:21:26,150 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 39945.75it/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.48k/1.48k [00:00<00:00, 203kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.95it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 81.2MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 49.1MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  85% 41.9M/49.6M [00:00<00:00, 49.7MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:01<00:00, 48.7MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 57.22it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8203\n",
            "  eval_loss               =     0.6758\n",
            "  eval_runtime            = 0:00:01.00\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    399.173\n",
            "  eval_steps_per_second   =     49.897\n",
            "100% 50/50 [00:00<00:00, 59.88it/s]\n",
            "Finished evaluation for language: min\n",
            "Running sentiment analysis evaluation for language: jav\n",
            "11/12/2024 19:21:41 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/jav to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/jav/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 81.8kB [00:00, 8.38MB/s]       \n",
            "Downloading data: 16.1kB [00:00, 11.8MB/s]       \n",
            "Downloading data: 65.8kB [00:00, 15.8MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/jav/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 891.52it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:21:47,815 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:21:47,815 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 11865.07it/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.48k/1.48k [00:00<00:00, 204kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  4.31it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:01, 20.1MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 35.9MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 49.6MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 50.7MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.09it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 57.20it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8422\n",
            "  eval_loss               =     0.4641\n",
            "  eval_runtime            = 0:00:00.99\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    402.522\n",
            "  eval_steps_per_second   =     50.315\n",
            "100% 50/50 [00:00<00:00, 60.11it/s]\n",
            "Finished evaluation for language: jav\n",
            "Running sentiment analysis evaluation for language: sun\n",
            "11/12/2024 19:22:03 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset nusa_x-senti/sun to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/sun/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5...\n",
            "Downloading data: 83.6kB [00:00, 8.39MB/s]       \n",
            "Downloading data: 16.5kB [00:00, 9.75MB/s]       \n",
            "Downloading data: 67.2kB [00:00, 19.8MB/s]       \n",
            "Dataset nusa_x-senti downloaded and prepared to /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/sun/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 667.32it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:22:09,798 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:22:09,798 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 36002.61it/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.48k/1.48k [00:00<00:00, 181kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  4.46it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 65.6MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 74.3MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 38.5MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  85% 41.9M/49.6M [00:00<00:00, 49.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 52.4MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.30it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 56.93it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8582\n",
            "  eval_loss               =     0.4735\n",
            "  eval_runtime            = 0:00:01.00\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    399.735\n",
            "  eval_steps_per_second   =     49.967\n",
            "100% 50/50 [00:00<00:00, 59.88it/s]\n",
            "Finished evaluation for language: sun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running SA evaluation for languages using our finetuned task SFT model.\n",
        "\n",
        " If you want to load a model, upload the model to collab and then replace /content/composable-sft/examples/text-classification/models/sa/smsa-trimmed with the model path. Our trained model for SA can be found at https://drive.google.com/drive/folders/1ijEWUZw6e34wv7eZRggRV0AvJ0aUoj8e?usp=sharing"
      ],
      "metadata": {
        "id": "-qG8aOrfApPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set PYTHONPATH for the src directory\n",
        "%env PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
        "\n",
        "# List of language codes to evaluate for sentiment analysis, excluding \"bbc\", \"bug\", and \"nij\"\n",
        "languages = [\"ace\", \"ban\", \"bjn\", \"eng\", \"ind\", \"mad\", \"min\", \"jav\", \"sun\"]\n",
        "\n",
        "# Loop through each language code and run eval_sa.sh with that language\n",
        "for lang in languages:\n",
        "    print(f\"Running sentiment analysis evaluation for language: {lang}\")\n",
        "    # Run the script with the language code as an argument\n",
        "    !source activate mlenv && ./eval_sa.sh {lang} /content/composable-sft/examples/text-classification/models/sa/smsa-trimmed\n",
        "    print(f\"Finished evaluation for language: {lang}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeCTfSkaxvsK",
        "outputId": "e4b8ce5e-8a22-4eb8-98ed-9ba925b45e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
            "Running sentiment analysis evaluation for language: ace\n",
            "11/12/2024 19:22:34 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:22:34 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ace/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 771.53it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:22:38,911 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:22:38,911 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 19508.39it/s]\n",
            "11/12/2024 19:22:46 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ace/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-d5e456596ff6a18f.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 56.85it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.7839\n",
            "  eval_loss               =     0.6262\n",
            "  eval_runtime            = 0:00:01.00\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =     399.37\n",
            "  eval_steps_per_second   =     49.921\n",
            "100% 50/50 [00:00<00:00, 59.45it/s]\n",
            "Finished evaluation for language: ace\n",
            "Running sentiment analysis evaluation for language: ban\n",
            "11/12/2024 19:22:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:22:53 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ban/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 704.81it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:22:57,576 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:22:57,576 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6759.56it/s]\n",
            "11/12/2024 19:23:04 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ban/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-a2342486a6b521dd.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 58.10it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8308\n",
            "  eval_loss               =     0.5237\n",
            "  eval_runtime            = 0:00:00.98\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =     407.61\n",
            "  eval_steps_per_second   =     50.951\n",
            "100% 50/50 [00:00<00:00, 59.86it/s]\n",
            "Finished evaluation for language: ban\n",
            "Running sentiment analysis evaluation for language: bjn\n",
            "11/12/2024 19:23:11 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:23:12 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/bjn/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 795.38it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:23:16,170 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:23:16,170 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 7115.02it/s]\n",
            "11/12/2024 19:23:23 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/bjn/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-e13acdaf455990ca.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 56.25it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8248\n",
            "  eval_loss               =     0.6689\n",
            "  eval_runtime            = 0:00:01.00\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    396.361\n",
            "  eval_steps_per_second   =     49.545\n",
            "100% 50/50 [00:00<00:00, 60.03it/s]\n",
            "Finished evaluation for language: bjn\n",
            "Running sentiment analysis evaluation for language: eng\n",
            "11/12/2024 19:23:30 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:23:30 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/eng/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 818.99it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:23:35,446 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:23:35,447 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 259, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-eng-small/revision/main\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 506, in <module>\n",
            "    main()\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 367, in main\n",
            "    lang_ft = SFT(sft_args.lang_ft)\n",
            "  File \"/content/composable-sft/src/sft/sft.py\", line 79, in __init__\n",
            "    sft_dir = pull_from_hf_model_hub(\n",
            "  File \"/content/composable-sft/src/sft/hf_utils.py\", line 5, in pull_from_hf_model_hub\n",
            "    return snapshot_download(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py\", line 183, in snapshot_download\n",
            "    repo_info = _api.repo_info(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1814, in repo_info\n",
            "    return method(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1624, in model_info\n",
            "    hf_raise_for_status(r)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 291, in hf_raise_for_status\n",
            "    raise RepositoryNotFoundError(message, response) from e\n",
            "huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6733ab3b-06b80fba2953e64028fcbcff)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-eng-small/revision/main.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "Invalid username or password.\n",
            "Finished evaluation for language: eng\n",
            "Running sentiment analysis evaluation for language: ind\n",
            "11/12/2024 19:23:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:23:44 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/ind/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 787.17it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:23:48,627 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:23:48,628 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 259, in hf_raise_for_status\n",
            "    response.raise_for_status()\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/requests/models.py\", line 1021, in raise_for_status\n",
            "    raise HTTPError(http_error_msg, response=self)\n",
            "requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-ind-small/revision/main\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 506, in <module>\n",
            "    main()\n",
            "  File \"/content/composable-sft/examples/text-classification/run_text_classification.py\", line 367, in main\n",
            "    lang_ft = SFT(sft_args.lang_ft)\n",
            "  File \"/content/composable-sft/src/sft/sft.py\", line 79, in __init__\n",
            "    sft_dir = pull_from_hf_model_hub(\n",
            "  File \"/content/composable-sft/src/sft/hf_utils.py\", line 5, in pull_from_hf_model_hub\n",
            "    return snapshot_download(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/_snapshot_download.py\", line 183, in snapshot_download\n",
            "    repo_info = _api.repo_info(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1814, in repo_info\n",
            "    return method(\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 120, in _inner_fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/hf_api.py\", line 1624, in model_info\n",
            "    hf_raise_for_status(r)\n",
            "  File \"/usr/local/envs/mlenv/lib/python3.9/site-packages/huggingface_hub/utils/_errors.py\", line 291, in hf_raise_for_status\n",
            "    raise RepositoryNotFoundError(message, response) from e\n",
            "huggingface_hub.utils._errors.RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-6733ab49-4603edcf78bb1fa01e730c38)\n",
            "\n",
            "Repository Not Found for url: https://huggingface.co/api/models/cambridgeltl/xlmr-lang-sft-ind-small/revision/main.\n",
            "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
            "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
            "Invalid username or password.\n",
            "Finished evaluation for language: ind\n",
            "Running sentiment analysis evaluation for language: mad\n",
            "11/12/2024 19:23:57 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:23:57 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/mad/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 755.23it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:24:01,815 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:24:01,816 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 11865.07it/s]\n",
            "11/12/2024 19:24:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/mad/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-397d830f17fa5357.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 57.34it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.7921\n",
            "  eval_loss               =     0.6775\n",
            "  eval_runtime            = 0:00:01.00\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    399.716\n",
            "  eval_steps_per_second   =     49.965\n",
            "100% 50/50 [00:00<00:00, 59.67it/s]\n",
            "Finished evaluation for language: mad\n",
            "Running sentiment analysis evaluation for language: min\n",
            "11/12/2024 19:24:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:24:16 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/min/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 830.99it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:24:20,201 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:24:20,201 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6932.73it/s]\n",
            "11/12/2024 19:24:27 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/min/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-b5a6c56e601db209.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 57.77it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8353\n",
            "  eval_loss               =     0.6648\n",
            "  eval_runtime            = 0:00:00.99\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    402.062\n",
            "  eval_steps_per_second   =     50.258\n",
            "100% 50/50 [00:00<00:00, 60.05it/s]\n",
            "Finished evaluation for language: min\n",
            "Running sentiment analysis evaluation for language: jav\n",
            "11/12/2024 19:24:34 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:24:34 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/jav/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 806.55it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:24:39,005 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:24:39,005 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 18396.07it/s]\n",
            "11/12/2024 19:24:46 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/jav/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-291a64dd255597cf.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 58.11it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =     0.8427\n",
            "  eval_loss               =     0.5204\n",
            "  eval_runtime            = 0:00:00.98\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    406.372\n",
            "  eval_steps_per_second   =     50.797\n",
            "100% 50/50 [00:00<00:00, 60.09it/s]\n",
            "Finished evaluation for language: jav\n",
            "Running sentiment analysis evaluation for language: sun\n",
            "11/12/2024 19:24:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/12/2024 19:24:53 - WARNING - datasets.builder - Found cached dataset nusa_x-senti (/root/.cache/huggingface/datasets/indonlp___nusa_x-senti/sun/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5)\n",
            "100% 3/3 [00:00<00:00, 812.53it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-12 19:24:57,982 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-12 19:24:57,982 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 12139.81it/s]\n",
            "11/12/2024 19:25:05 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/indonlp___nusa_x-senti/sun/1.0.0/3477a395c5c7a09a74d897ceb96ebd2c3afbd1d7fad0c11d8b8026b8b08e3af5/cache-b9f7a94abd0f7f62.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 50/50 [00:00<00:00, 56.54it/s]\n",
            "***** eval metrics *****\n",
            "  eval_f1                 =       0.85\n",
            "  eval_loss               =     0.5206\n",
            "  eval_runtime            = 0:00:01.00\n",
            "  eval_samples            =        400\n",
            "  eval_samples_per_second =    398.386\n",
            "  eval_steps_per_second   =     49.798\n",
            "100% 50/50 [00:00<00:00, 60.39it/s]\n",
            "Finished evaluation for language: sun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running evaluation for NLI task using our finetuned task SFT model.\n",
        "\n",
        "For the NLI task we trained the model separately on a GPU server as collab was unreliable due to its large training time(the command used was ./train_nli.sh). Here we mount the model from the drive. To access the NLI task SFT model use the drive link: https://drive.google.com/drive/folders/1BPxlTlDNJHqeIsUrBw5P8eLMY5u68JDN?usp=sharing and copy it to collab. After copying, replace /content/composable-sft/examples/text-classification/models/NLI-SFT with the correct path."
      ],
      "metadata": {
        "id": "Y7vCxru72-2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code would have trained the nli SFT model, but we decided to mount it from our drive as training was unstable on collab"
      ],
      "metadata": {
        "id": "2CZM6GRhywof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %env PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
        "# !source activate mlenv && ./train_nli.sh"
      ],
      "metadata": {
        "id": "tDt-ASmQys2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp -r /content/drive/MyDrive/NLI-SFT /content/composable-sft/examples/text-classification/models/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM3axt-E3xb8",
        "outputId": "72cadf3c-abb5-411b-99d0-0e9e20aa42e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running NLI evaluation for languages using our trained model."
      ],
      "metadata": {
        "id": "qi3zF0_RBDak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set PYTHONPATH for the src directory\n",
        "%env PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
        "\n",
        "# List of language codes to evaluate\n",
        "languages = [\"aym\", \"bzd\", \"cni\", \"gn\", \"hch\", \"nah\", \"oto\", \"quy\", \"shp\", \"tar\"]\n",
        "\n",
        "# Loop through each language code and run eval_nli.sh with that language\n",
        "for lang in languages:\n",
        "    print(f\"Running evaluation for language: {lang}\")\n",
        "    # Run the script with the language code as an argument\n",
        "    !source activate mlenv && ./eval_nli.sh {lang} /content/composable-sft/examples/text-classification/models/NLI-SFT\n",
        "    print(f\"Finished evaluation for language: {lang}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ0N0C3jBzvY",
        "outputId": "130dacc7-5e41-4e55-ed9a-5b8c3ad16072"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
            "Running evaluation for language: aym\n",
            "11/13/2024 00:25:50 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-72d19e0e14e47eaf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 6657.63it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1228.20it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-72d19e0e14e47eaf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 586.53it/s]\n",
            "Downloading config.json: 100% 615/615 [00:00<00:00, 75.3kB/s]\n",
            "Downloading tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 3.66kB/s]\n",
            "Downloading (…)tencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 18.1MB/s]\n",
            "Downloading tokenizer.json: 100% 9.10M/9.10M [00:00<00:00, 18.7MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.12G/1.12G [00:12<00:00, 86.2MB/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:26:09,519 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:26:09,519 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 72.1MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 59.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 69.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  85% 41.9M/49.6M [00:00<00:00, 45.9MB/s]\u001b[A\n",
            "\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 169kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:01<00:01,  1.43s/it]\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:01<00:00, 48.0MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.24it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "Downloading builder script: 2.94kB [00:00, 2.11MB/s]       \n",
            "100% 94/94 [00:02<00:00, 38.66it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5747\n",
            "  eval_loss               =     0.9889\n",
            "  eval_runtime            = 0:00:02.54\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    294.619\n",
            "  eval_steps_per_second   =     36.926\n",
            "100% 94/94 [00:02<00:00, 38.33it/s]\n",
            "Finished evaluation for language: aym\n",
            "Running evaluation for language: bzd\n",
            "11/13/2024 00:26:29 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7ca2bd447ed6a4ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 7025.63it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1266.40it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7ca2bd447ed6a4ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 818.40it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:26:33,971 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:26:33,971 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 172kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.55it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 80.8MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 82.2MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 84.7MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 85.0MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.23it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 38.36it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.424\n",
            "  eval_loss               =     1.4108\n",
            "  eval_runtime            = 0:00:02.57\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    291.446\n",
            "  eval_steps_per_second   =     36.528\n",
            "100% 94/94 [00:02<00:00, 38.17it/s]\n",
            "Finished evaluation for language: bzd\n",
            "Running evaluation for language: cni\n",
            "11/13/2024 00:26:52 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-25fd7c0cdbd6269b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 6775.94it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1245.34it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-25fd7c0cdbd6269b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 796.94it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:26:57,131 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:26:57,131 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 173kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.95it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 71.3MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 86.6MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 88.6MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 88.3MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.37it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 38.58it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =       0.46\n",
            "  eval_loss               =     1.0832\n",
            "  eval_runtime            = 0:00:02.55\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    293.606\n",
            "  eval_steps_per_second   =     36.799\n",
            "100% 94/94 [00:02<00:00, 38.39it/s]\n",
            "Finished evaluation for language: cni\n",
            "Running evaluation for language: gn\n",
            "11/13/2024 00:27:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-ac02e3ba40724ac8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 6909.89it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1212.23it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-ac02e3ba40724ac8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 819.52it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:27:19,938 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:27:19,939 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 170kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  4.04it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 64.5MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 89.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 87.2MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.23it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 38.34it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.628\n",
            "  eval_loss               =     0.9208\n",
            "  eval_runtime            = 0:00:02.56\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    291.896\n",
            "  eval_steps_per_second   =     36.584\n",
            "100% 94/94 [00:02<00:00, 38.25it/s]\n",
            "Finished evaluation for language: gn\n",
            "Running evaluation for language: hch\n",
            "11/13/2024 00:27:38 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1b5627fa1821a408/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 6898.53it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1238.35it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1b5627fa1821a408/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 747.51it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:27:42,860 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:27:42,860 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 170kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.99it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 73.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 78.2MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 70.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 79.0MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.17it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 38.46it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4053\n",
            "  eval_loss               =     1.4071\n",
            "  eval_runtime            = 0:00:02.56\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    292.725\n",
            "  eval_steps_per_second   =     36.688\n",
            "100% 94/94 [00:02<00:00, 37.91it/s]\n",
            "Finished evaluation for language: hch\n",
            "Running evaluation for language: nah\n",
            "11/13/2024 00:28:01 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-99a49a3772660db9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 6069.90it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1195.30it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-99a49a3772660db9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 809.71it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:28:05,848 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:28:05,848 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 166kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.80it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 45.7MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 96.2MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 88.8MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.35it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 93/93 [00:02<00:00, 38.32it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5108\n",
            "  eval_loss               =     1.0639\n",
            "  eval_runtime            = 0:00:02.54\n",
            "  eval_samples            =        738\n",
            "  eval_samples_per_second =    289.948\n",
            "  eval_steps_per_second   =     36.538\n",
            "100% 93/93 [00:02<00:00, 38.01it/s]\n",
            "Finished evaluation for language: nah\n",
            "Running evaluation for language: oto\n",
            "11/13/2024 00:28:24 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-cdd02c2168681b2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 6168.09it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1203.19it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-cdd02c2168681b2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 793.77it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:28:28,855 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:28:28,856 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 165kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.63it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 83.3MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 85.7MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 88.1MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 89.1MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.04it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.97it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.393\n",
            "  eval_loss               =     1.0931\n",
            "  eval_runtime            = 0:00:02.59\n",
            "  eval_samples            =        748\n",
            "  eval_samples_per_second =    288.535\n",
            "  eval_steps_per_second   =      36.26\n",
            "100% 94/94 [00:02<00:00, 36.72it/s]\n",
            "Finished evaluation for language: oto\n",
            "Running evaluation for language: quy\n",
            "11/13/2024 00:28:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-709fdf193108d0c9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 7025.63it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1107.85it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-709fdf193108d0c9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 807.68it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:28:52,150 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:28:52,150 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 166kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.66it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 61.7MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 89.7MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 87.3MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.28it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 38.27it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6093\n",
            "  eval_loss               =     1.0386\n",
            "  eval_runtime            = 0:00:02.57\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    291.113\n",
            "  eval_steps_per_second   =     36.486\n",
            "100% 94/94 [00:02<00:00, 38.07it/s]\n",
            "Finished evaluation for language: quy\n",
            "Running evaluation for language: shp\n",
            "11/13/2024 00:29:10 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7a512539720ecd6e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 5108.77it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 997.69it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7a512539720ecd6e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 825.81it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:29:15,123 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:29:15,123 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 168kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.41it/s]\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 42.9MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 97.2MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 89.0MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.39it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 38.18it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.496\n",
            "  eval_loss               =     1.1904\n",
            "  eval_runtime            = 0:00:02.58\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    290.664\n",
            "  eval_steps_per_second   =      36.43\n",
            "100% 94/94 [00:02<00:00, 38.01it/s]\n",
            "Finished evaluation for language: shp\n",
            "Running evaluation for language: tar\n",
            "11/13/2024 00:29:33 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-01c6a4b331bacf72/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 7231.56it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1277.19it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-01c6a4b331bacf72/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 817.44it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:29:38,072 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:29:38,072 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 169kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.91it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/49.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  21% 10.5M/49.6M [00:00<00:00, 80.6MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  42% 21.0M/49.6M [00:00<00:00, 81.9MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  63% 31.5M/49.6M [00:00<00:00, 84.3MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 49.6M/49.6M [00:00<00:00, 83.3MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00,  2.29it/s]\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.85it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4173\n",
            "  eval_loss               =     1.3821\n",
            "  eval_runtime            = 0:00:02.60\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    288.037\n",
            "  eval_steps_per_second   =     36.101\n",
            "100% 94/94 [00:02<00:00, 37.58it/s]\n",
            "Finished evaluation for language: tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running NLI evaluation for languages using the author provided model."
      ],
      "metadata": {
        "id": "yYHdtQ3DBGri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set PYTHONPATH for the src directory\n",
        "%env PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
        "\n",
        "# List of language codes to evaluate\n",
        "languages = [\"aym\", \"bzd\", \"cni\", \"gn\", \"hch\", \"nah\", \"oto\", \"quy\", \"shp\", \"tar\"]\n",
        "\n",
        "# Loop through each language code and run eval_nli.sh with that language\n",
        "for lang in languages:\n",
        "    print(f\"Running evaluation for language: {lang}\")\n",
        "    # Run the script with the language code as an argument\n",
        "    !source activate mlenv && ./eval_nli.sh {lang}\n",
        "    print(f\"Finished evaluation for language: {lang}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcTjpmDgwdPQ",
        "outputId": "fce5b7b6-8164-4b17-f753-b2d565bc702a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=/content/composable-sft/src:$PYTHONPATH\n",
            "Running evaluation for language: aym\n",
            "11/13/2024 00:29:56 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:29:56 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-72d19e0e14e47eaf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 654.13it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:30:01,020 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:30:01,020 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading .gitattributes: 100% 1.18k/1.18k [00:00<00:00, 154kB/s]\n",
            "Fetching 2 files:  50% 1/2 [00:00<00:00,  3.23it/s]\n",
            "Downloading pytorch_diff.bin:   0% 0.00/87.6M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  12% 10.5M/87.6M [00:00<00:00, 79.9MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  24% 21.0M/87.6M [00:00<00:00, 84.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  36% 31.5M/87.6M [00:00<00:00, 87.4MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  48% 41.9M/87.6M [00:00<00:00, 86.8MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  60% 52.4M/87.6M [00:00<00:00, 86.9MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  72% 62.9M/87.6M [00:00<00:00, 86.8MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin:  84% 73.4M/87.6M [00:00<00:00, 86.1MB/s]\u001b[A\n",
            "Downloading pytorch_diff.bin: 100% 87.6M/87.6M [00:00<00:00, 88.2MB/s]\n",
            "Fetching 2 files: 100% 2/2 [00:01<00:00,  1.52it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6452.78it/s]\n",
            "11/13/2024 00:30:09 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-72d19e0e14e47eaf/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-deb798873b1db151.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.91it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5813\n",
            "  eval_loss               =     0.9905\n",
            "  eval_runtime            = 0:00:02.59\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    288.693\n",
            "  eval_steps_per_second   =     36.183\n",
            "100% 94/94 [00:02<00:00, 37.83it/s]\n",
            "Finished evaluation for language: aym\n",
            "Running evaluation for language: bzd\n",
            "11/13/2024 00:30:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:30:20 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-7ca2bd447ed6a4ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 577.49it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:30:24,639 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:30:24,639 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 15448.63it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6345.39it/s]\n",
            "11/13/2024 00:30:31 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-7ca2bd447ed6a4ec/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-da2d6534d45cc657.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.72it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.444\n",
            "  eval_loss               =     1.2455\n",
            "  eval_runtime            = 0:00:02.61\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    287.066\n",
            "  eval_steps_per_second   =     35.979\n",
            "100% 94/94 [00:02<00:00, 37.53it/s]\n",
            "Finished evaluation for language: bzd\n",
            "Running evaluation for language: cni\n",
            "11/13/2024 00:30:42 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:30:42 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-25fd7c0cdbd6269b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 637.72it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:30:47,010 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:30:47,011 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 33288.13it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 3795.75it/s]\n",
            "11/13/2024 00:30:54 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-25fd7c0cdbd6269b/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-a2c2865fb8b1f48e.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.89it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4787\n",
            "  eval_loss               =      1.111\n",
            "  eval_runtime            = 0:00:02.59\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    288.622\n",
            "  eval_steps_per_second   =     36.174\n",
            "100% 94/94 [00:02<00:00, 37.94it/s]\n",
            "Finished evaluation for language: cni\n",
            "Running evaluation for language: gn\n",
            "11/13/2024 00:31:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:31:05 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-ac02e3ba40724ac8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 600.13it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:31:09,341 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:31:09,342 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 27324.46it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6074.30it/s]\n",
            "11/13/2024 00:31:16 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-ac02e3ba40724ac8/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-9f1af676cf456a2d.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.53it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.6347\n",
            "  eval_loss               =     0.9394\n",
            "  eval_runtime            = 0:00:02.62\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    285.977\n",
            "  eval_steps_per_second   =     35.842\n",
            "100% 94/94 [00:02<00:00, 37.18it/s]\n",
            "Finished evaluation for language: gn\n",
            "Running evaluation for language: hch\n",
            "11/13/2024 00:31:27 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:31:27 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-1b5627fa1821a408/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 644.48it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:31:31,561 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:31:31,561 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 34239.22it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 27685.17it/s]\n",
            "11/13/2024 00:31:38 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-1b5627fa1821a408/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-e7e7a4aecb2fa1c4.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.20it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =      0.428\n",
            "  eval_loss               =     1.3505\n",
            "  eval_runtime            = 0:00:02.65\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    282.848\n",
            "  eval_steps_per_second   =      35.45\n",
            "100% 94/94 [00:02<00:00, 37.00it/s]\n",
            "Finished evaluation for language: hch\n",
            "Running evaluation for language: nah\n",
            "11/13/2024 00:31:49 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:31:50 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-99a49a3772660db9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 600.22it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:31:54,302 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:31:54,302 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 33026.02it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6615.62it/s]\n",
            "11/13/2024 00:32:01 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-99a49a3772660db9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-62fda4ad17d559a5.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 93/93 [00:02<00:00, 37.70it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5244\n",
            "  eval_loss               =     1.0414\n",
            "  eval_runtime            = 0:00:02.58\n",
            "  eval_samples            =        738\n",
            "  eval_samples_per_second =    285.218\n",
            "  eval_steps_per_second   =     35.942\n",
            "100% 93/93 [00:02<00:00, 37.65it/s]\n",
            "Finished evaluation for language: nah\n",
            "Running evaluation for language: oto\n",
            "11/13/2024 00:32:12 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:32:12 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-cdd02c2168681b2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 603.58it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:32:16,823 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:32:16,823 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 19195.90it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6684.15it/s]\n",
            "11/13/2024 00:32:24 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-cdd02c2168681b2f/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-628539c1b96bbbbd.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.62it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4853\n",
            "  eval_loss               =     1.0774\n",
            "  eval_runtime            = 0:00:02.61\n",
            "  eval_samples            =        748\n",
            "  eval_samples_per_second =    285.915\n",
            "  eval_steps_per_second   =      35.93\n",
            "100% 94/94 [00:02<00:00, 37.44it/s]\n",
            "Finished evaluation for language: oto\n",
            "Running evaluation for language: quy\n",
            "11/13/2024 00:32:34 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:32:35 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-709fdf193108d0c9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 644.09it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:32:39,054 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:32:39,055 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 41943.04it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 4581.44it/s]\n",
            "11/13/2024 00:32:46 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-709fdf193108d0c9/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d4806b27083189d3.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.59it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =       0.62\n",
            "  eval_loss               =     1.0602\n",
            "  eval_runtime            = 0:00:02.62\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    286.114\n",
            "  eval_steps_per_second   =      35.86\n",
            "100% 94/94 [00:02<00:00, 37.35it/s]\n",
            "Finished evaluation for language: quy\n",
            "Running evaluation for language: shp\n",
            "11/13/2024 00:32:57 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:32:57 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-7a512539720ecd6e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 670.45it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:33:01,351 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:33:01,351 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 18682.87it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 6517.95it/s]\n",
            "11/13/2024 00:33:08 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-7a512539720ecd6e/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-72360c7806e74bc9.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.26it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.5027\n",
            "  eval_loss               =     1.3804\n",
            "  eval_runtime            = 0:00:02.64\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    283.751\n",
            "  eval_steps_per_second   =     35.563\n",
            "100% 94/94 [00:02<00:00, 37.35it/s]\n",
            "Finished evaluation for language: shp\n",
            "Running evaluation for language: tar\n",
            "11/13/2024 00:33:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/13/2024 00:33:19 - WARNING - datasets.builder - Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-01c6a4b331bacf72/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
            "100% 1/1 [00:00<00:00, 658.96it/s]\n",
            "[WARNING|modeling_utils.py:3180] 2024-11-13 00:33:23,459 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:3192] 2024-11-13 00:33:23,459 >> Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 4793.49it/s]\n",
            "Fetching 2 files: 100% 2/2 [00:00<00:00, 11983.73it/s]\n",
            "11/13/2024 00:33:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-01c6a4b331bacf72/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-95edcc2245a41b08.arrow\n",
            "/content/composable-sft/examples/text-classification/run_text_classification.py:416: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = load_metric(data_args.eval_metric)\n",
            "100% 94/94 [00:02<00:00, 37.61it/s]\n",
            "***** eval metrics *****\n",
            "  eval_accuracy           =     0.4333\n",
            "  eval_loss               =     1.4084\n",
            "  eval_runtime            = 0:00:02.61\n",
            "  eval_samples            =        750\n",
            "  eval_samples_per_second =    286.616\n",
            "  eval_steps_per_second   =     35.923\n",
            "100% 94/94 [00:02<00:00, 37.05it/s]\n",
            "Finished evaluation for language: tar\n"
          ]
        }
      ]
    }
  ]
}